
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Architectures &#8212; ANTsPyNet 0.0.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Utilities" href="utilities.html" />
    <link rel="prev" title="ANTsPyNet documentation" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="architectures">
<h1>Architectures<a class="headerlink" href="#architectures" title="Permalink to this headline">¶</a></h1>
<div class="section" id="image-voxelwise-segmentation-regression">
<h2>Image voxelwise segmentation/regression<a class="headerlink" href="#image-voxelwise-segmentation-regression" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="antspynet.architectures.create_unet_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_unet_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_outputs=1</em>, <em>number_of_layers=4</em>, <em>number_of_filters_at_base_layer=32</em>, <em>convolution_kernel_size=(3</em>, <em>3)</em>, <em>deconvolution_kernel_size=(2</em>, <em>2)</em>, <em>pool_size=(2</em>, <em>2)</em>, <em>strides=(2</em>, <em>2)</em>, <em>dropout_rate=0.0</em>, <em>weight_decay=0.0</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_unet_model.html#create_unet_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_unet_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the U-net deep learning architecture.</p>
<p>Creates a keras model of the U-net deep learning architecture for image
segmentation and regression.  More information is provided at the authors’
website:</p>
<blockquote>
<div><a class="reference external" href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/</a></div></blockquote>
<p>with the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/joelthelion/ultrasound-nerve-segmentation">https://github.com/joelthelion/ultrasound-nerve-segmentation</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_outputs</strong> (<em>python:integer</em>) – Meaning depends on the mode.  For <cite>classification</cite> this is the number of
segmentation labels.  For <cite>regression</cite> this is the number of outputs.</li>
<li><strong>number_of_layers</strong> (<em>python:integer</em>) – number of encoding/decoding layers.</li>
<li><strong>number_of_filters_at_base_layer</strong> (<em>python:integer</em>) – number of filters at the beginning and end of the <cite>U</cite>.  Doubles at each
descending/ascending layer.</li>
<li><strong>convolution_kernel_size</strong> (<em>tuple of length 2</em>) – Defines the kernel size during the encoding.</li>
<li><strong>deconvolution_kernel_size</strong> (<em>tuple of length 2</em>) – Defines the kernel size during the decoding.</li>
<li><strong>pool_size</strong> (<em>tuple of length 2</em>) – Defines the region for each pooling layer.</li>
<li><strong>strides</strong> (<em>tuple of length 2</em>) – Strides for the convolutional layers.</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Float between 0 and 1 to use between dense layers.</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weighting parameter for L2 regularization of the kernel weights of the
convolution layers.  Default = 0.0.</li>
<li><strong>mode</strong> (<em>string</em>) – <cite>classification</cite> or <cite>regression</cite>.  Default = <cite>classification</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D keras model defining the U-net network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_unet_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_unet_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_unet_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_outputs=1</em>, <em>number_of_layers=4</em>, <em>number_of_filters_at_base_layer=32</em>, <em>convolution_kernel_size=(3</em>, <em>3</em>, <em>3)</em>, <em>deconvolution_kernel_size=(2</em>, <em>2</em>, <em>2)</em>, <em>pool_size=(2</em>, <em>2</em>, <em>2)</em>, <em>strides=(2</em>, <em>2</em>, <em>2)</em>, <em>dropout_rate=0.0</em>, <em>weight_decay=0.0</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_unet_model.html#create_unet_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_unet_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the U-net deep learning architecture.</p>
<p>Creates a keras model of the U-net deep learning architecture for image
segmentation and regression.  More information is provided at the authors’
website:</p>
<blockquote>
<div><a class="reference external" href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/</a></div></blockquote>
<p>with the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/joelthelion/ultrasound-nerve-segmentation">https://github.com/joelthelion/ultrasound-nerve-segmentation</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_outputs</strong> (<em>python:integer</em>) – Meaning depends on the mode.  For <cite>classification</cite> this is the number of
segmentation labels.  For <cite>regression</cite> this is the number of outputs.</li>
<li><strong>number_of_layers</strong> (<em>python:integer</em>) – number of encoding/decoding layers.</li>
<li><strong>number_of_filters_at_base_layer</strong> (<em>python:integer</em>) – number of filters at the beginning and end of the <cite>U</cite>.  Doubles at each
descending/ascending layer.</li>
<li><strong>convolution_kernel_size</strong> (<em>tuple of length 4</em>) – Defines the kernel size during the encoding.</li>
<li><strong>deconvolution_kernel_size</strong> (<em>tuple of length 4</em>) – Defines the kernel size during the decoding.</li>
<li><strong>pool_size</strong> (<em>tuple of length 4</em>) – Defines the region for each pooling layer.</li>
<li><strong>strides</strong> (<em>tuple of length 4</em>) – Strides for the convolutional layers.</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Float between 0 and 1 to use between dense layers.</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weighting parameter for L2 regularization of the kernel weights of the
convolution layers.  Default = 0.0.</li>
<li><strong>mode</strong> (<em>string</em>) – <cite>classification</cite> or <cite>regression</cite>.  Default = <cite>classification</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D keras model defining the U-net network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_unet_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_resunet_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_resunet_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_outputs=1</em>, <em>number_of_filters_at_base_layer=32</em>, <em>bottle_neck_block_depth_schedule=(3</em>, <em>4)</em>, <em>convolution_kernel_size=(3</em>, <em>3)</em>, <em>deconvolution_kernel_size=(2</em>, <em>2)</em>, <em>dropout_rate=0.0</em>, <em>weight_decay=0.0</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_resunet_model.html#create_resunet_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_resunet_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the Resnet + U-net deep learning architecture.</p>
<p>Creates a keras model of the U-net + ResNet deep learning architecture for
image segmentation and regression with the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1608.04117">https://arxiv.org/abs/1608.04117</a></div></blockquote>
<p>This particular implementation was ported from the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/veugene/fcn_maker/">https://github.com/veugene/fcn_maker/</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The
shape (or dimension) of that tensor is the image dimensions followed by
the number of channels (e.g., red, green, and blue).  The batch size
(i.e., number of training images) is not specified a priori.</li>
<li><strong>number_of_outputs</strong> (<em>python:integer</em>) – Meaning depends on the mode.  For ‘classification’ this is the number of
segmentation labels.  For ‘regression’ this is the number of outputs.</li>
<li><strong>number_of_filters_at_base_layer</strong> (<em>python:integer</em>) – Number of filters at the beginning and end of the ‘U’.  Doubles at each
descending/ascending layer.</li>
<li><strong>bottle_neck_block_depth_schedule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Tuple that provides the encoding layer schedule for the number of bottleneck
blocks per long skip connection.</li>
<li><strong>convolution_kernel_size</strong> (<em>tuple of length 2</em>) – 2-d vector defining the kernel size during the encoding path</li>
<li><strong>deconvolution_kernel_size</strong> (<em>tuple of length 2</em>) – 2-d vector defining the kernel size during the decoding</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Float between 0 and 1 to use between dense layers.</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weighting parameter for L2 regularization of the kernel weights of the
convolution layers.  Default = 0.0.</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_resunet_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_resunet_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_resunet_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_outputs=1</em>, <em>number_of_filters_at_base_layer=32</em>, <em>bottle_neck_block_depth_schedule=(3</em>, <em>4)</em>, <em>convolution_kernel_size=(3</em>, <em>3</em>, <em>3)</em>, <em>deconvolution_kernel_size=(2</em>, <em>2</em>, <em>2)</em>, <em>dropout_rate=0.0</em>, <em>weight_decay=0.0</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_resunet_model.html#create_resunet_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_resunet_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the Resnet + U-net deep learning architecture.</p>
<p>Creates a keras model of the U-net + ResNet deep learning architecture for
image segmentation and regression with the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1608.04117">https://arxiv.org/abs/1608.04117</a></div></blockquote>
<p>This particular implementation was ported from the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/veugene/fcn_maker/">https://github.com/veugene/fcn_maker/</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The
shape (or dimension) of that tensor is the image dimensions followed by
the number of channels (e.g., red, green, and blue).  The batch size
(i.e., number of training images) is not specified a priori.</li>
<li><strong>number_of_outputs</strong> (<em>python:integer</em>) – Meaning depends on the mode.  For ‘classification’ this is the number of
segmentation labels.  For ‘regression’ this is the number of outputs.</li>
<li><strong>number_of_filters_at_base_layer</strong> (<em>python:integer</em>) – Number of filters at the beginning and end of the ‘U’.  Doubles at each
descending/ascending layer.</li>
<li><strong>bottle_neck_block_depth_schedule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Tuple that provides the encoding layer schedule for the number of bottleneck
blocks per long skip connection.</li>
<li><strong>convolution_kernel_size</strong> (<em>tuple of length 3</em>) – 3-d vector defining the kernel size during the encoding path</li>
<li><strong>deconvolution_kernel_size</strong> (<em>tuple of length 3</em>) – 3-d vector defining the kernel size during the decoding</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Float between 0 and 1 to use between dense layers.</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weighting parameter for L2 regularization of the kernel weights of the
convolution layers.  Default = 0.0.</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_resunet_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_denseunet_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_denseunet_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_outputs=1</em>, <em>number_of_layers_per_dense_block=(6</em>, <em>12</em>, <em>36</em>, <em>24)</em>, <em>growth_rate=48</em>, <em>initial_number_of_filters=96</em>, <em>reduction_rate=0.0</em>, <em>depth=7</em>, <em>dropout_rate=0.0</em>, <em>weight_decay=0.0001</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_denseunet_model.html#create_denseunet_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_denseunet_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the dense U-net deep learning architecture.</p>
<p>Creates a keras model of the dense U-net deep learning architecture for
image segmentation</p>
<p>X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, P.-A. Heng. H-DenseUNet: Hybrid
Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes</p>
<p>available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/pdf/1709.07330.pdf">https://arxiv.org/pdf/1709.07330.pdf</a></div></blockquote>
<p>with the author’s implementation available at:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/xmengli999/H-DenseUNet">https://github.com/xmengli999/H-DenseUNet</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The
shape (or dimension) of that tensor is the image dimensions followed by
the number of channels (e.g., red, green, and blue).  The batch size
(i.e., number of training images) is not specified a priori.</li>
<li><strong>number_of_outputs</strong> (<em>python:integer</em>) – Meaning depends on the mode.  For ‘classification’ this is the number of
segmentation labels.  For ‘regression’ this is the number of outputs.</li>
<li><strong>number_of_layers_per_dense_blocks</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Number of dense blocks per layer.</li>
<li><strong>growth_rate</strong> (<em>python:integer</em>) – Number of filters to add for each dense block layer (default = 48).</li>
<li><strong>initial_number_of_filters</strong> (<em>python:integer</em>) – Number of filters at the beginning (default = 96).</li>
<li><strong>reduction_rate</strong> (<em>scalar</em>) – Reduction factor of transition blocks.</li>
<li><strong>depth</strong> (<em>python:integer</em>) – Number of layers—must be equal to 3 * N + 4 where N is an integer
(default = 7).</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Float between 0 and 1 to use between dense layers.</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weighting parameter for L2 regularization of the kernel weights of the
convolution layers (default = 1e-4).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_denseunet_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_denseunet_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_denseunet_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_outputs=1</em>, <em>number_of_layers_per_dense_block=(6</em>, <em>12</em>, <em>36</em>, <em>24)</em>, <em>growth_rate=48</em>, <em>initial_number_of_filters=96</em>, <em>reduction_rate=0.0</em>, <em>depth=7</em>, <em>dropout_rate=0.0</em>, <em>weight_decay=0.0001</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_denseunet_model.html#create_denseunet_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_denseunet_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the dense U-net deep learning architecture.</p>
<p>Creates a keras model of the dense U-net deep learning architecture for
image segmentation</p>
<p>X. Li, H. Chen, X. Qi, Q. Dou, C.-W. Fu, P.-A. Heng. H-DenseUNet: Hybrid
Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes</p>
<p>available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/pdf/1709.07330.pdf">https://arxiv.org/pdf/1709.07330.pdf</a></div></blockquote>
<p>with the author’s implementation available at:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/xmengli999/H-DenseUNet">https://github.com/xmengli999/H-DenseUNet</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The
shape (or dimension) of that tensor is the image dimensions followed by
the number of channels (e.g., red, green, and blue).  The batch size
(i.e., number of training images) is not specified a priori.</li>
<li><strong>number_of_outputs</strong> (<em>python:integer</em>) – Meaning depends on the mode.  For ‘classification’ this is the number of
segmentation labels.  For ‘regression’ this is the number of outputs.</li>
<li><strong>number_of_layers_per_dense_blocks</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Number of dense blocks per layer.</li>
<li><strong>growth_rate</strong> (<em>python:integer</em>) – Number of filters to add for each dense block layer (default = 48).</li>
<li><strong>initial_number_of_filters</strong> (<em>python:integer</em>) – Number of filters at the beginning (default = 96).</li>
<li><strong>reduction_rate</strong> (<em>scalar</em>) – Reduction factor of transition blocks.</li>
<li><strong>depth</strong> (<em>python:integer</em>) – Number of layers—must be equal to 3 * N + 4 where N is an integer
(default = 7).</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Float between 0 and 1 to use between dense layers.</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weighting parameter for L2 regularization of the kernel weights of the
convolution layers (default = 1e-4).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_denseunet_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="image-classification-regression">
<h2>Image classification/regression<a class="headerlink" href="#image-classification-regression" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="antspynet.architectures.create_alexnet_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_alexnet_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>number_of_dense_units=4096</em>, <em>dropout_rate=0.0</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_alexnet_model.html#create_alexnet_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_alexnet_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the AlexNet deep learning architecture.</p>
<p>Creates a keras model of the AlexNet deep learning architecture for image
recognition based on the paper</p>
<ol class="upperalpha simple">
<li>Krizhevsky, and I. Sutskever, and G. Hinton. ImageNet Classification with Deep Convolutional Neural Networks.</li>
</ol>
<p>available here:</p>
<blockquote>
<div><a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/duggalrahul/AlexNet-Experiments-Keras/">https://github.com/duggalrahul/AlexNet-Experiments-Keras/</a>
<a class="reference external" href="https://github.com/lunardog/convnets-keras/">https://github.com/lunardog/convnets-keras/</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The
shape (or dimension) of that tensor is the image dimensions followed by
the number of channels (e.g., red, green, and blue).  The batch size
(i.e., number of training images) is not specified a priori.</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of segmentation labels.</li>
<li><strong>number_of_dense_units</strong> (<em>python:integer</em>) – Number of dense units.</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Optional regularization parameter between [0, 1]. Default = 0.0.</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_alexnet_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_alexnet_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_alexnet_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>number_of_dense_units=4096</em>, <em>dropout_rate=0.0</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_alexnet_model.html#create_alexnet_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_alexnet_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the AlexNet deep learning architecture.</p>
<p>Creates a keras model of the AlexNet deep learning architecture for image
recognition based on the paper</p>
<ol class="upperalpha simple">
<li>Krizhevsky, and I. Sutskever, and G. Hinton. ImageNet Classification with Deep Convolutional Neural Networks.</li>
</ol>
<p>available here:</p>
<blockquote>
<div><a class="reference external" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/duggalrahul/AlexNet-Experiments-Keras/">https://github.com/duggalrahul/AlexNet-Experiments-Keras/</a>
<a class="reference external" href="https://github.com/lunardog/convnets-keras/">https://github.com/lunardog/convnets-keras/</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The
shape (or dimension) of that tensor is the image dimensions followed by
the number of channels (e.g., red, green, and blue).  The batch size
(i.e., number of training images) is not specified a priori.</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of segmentation labels.</li>
<li><strong>number_of_dense_units</strong> (<em>python:integer</em>) – Number of dense units.</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Optional regularization parameter between [0, 1]. Default = 0.0.</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_alexnet_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_vgg_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_vgg_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>layers=(1</em>, <em>2</em>, <em>3</em>, <em>4</em>, <em>4)</em>, <em>lowest_resolution=64</em>, <em>convolution_kernel_size=(3</em>, <em>3)</em>, <em>pool_size=(2</em>, <em>2)</em>, <em>strides=(2</em>, <em>2)</em>, <em>number_of_dense_units=4096</em>, <em>dropout_rate=0.0</em>, <em>style=19</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_vgg_model.html#create_vgg_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_vgg_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the Vgg deep learning architecture.</p>
<p>Creates a keras model of the Vgg deep learning architecture for image
recognition based on the paper</p>
<p>K. Simonyan and A. Zisserman, Very Deep Convolutional Networks for
Large-Scale Image Recognition</p>
<p>available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://gist.github.com/baraldilorenzo/8d096f48a1be4a2d660d">https://gist.github.com/baraldilorenzo/8d096f48a1be4a2d660d</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of classification labels.</li>
<li><strong>layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – A tuple determining the number of ‘filters’ defined at for each layer.</li>
<li><strong>lowest_resolution</strong> (<em>python:integer</em>) – Number of filters at the beginning.</li>
<li><strong>convolution_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – 2-d vector definining the kernel size during the encoding path</li>
<li><strong>pool_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – 2-d vector defining the region for each pooling layer.</li>
<li><strong>strides</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – 2-d vector describing the stride length in each direction.</li>
<li><strong>number_of_dense_units</strong> (<em>python:integer</em>) – Number of units in the last layers.</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Between 0 and 1 to use between dense layers.</li>
<li><strong>style</strong> (<em>python:integer</em>) – ‘16’ or ‘19’ for VGG16 or VGG19, respectively.</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_vgg_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_vgg_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_vgg_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>layers=(1</em>, <em>2</em>, <em>3</em>, <em>4</em>, <em>4)</em>, <em>lowest_resolution=64</em>, <em>convolution_kernel_size=(3</em>, <em>3</em>, <em>3)</em>, <em>pool_size=(2</em>, <em>2</em>, <em>2)</em>, <em>strides=(2</em>, <em>2</em>, <em>2)</em>, <em>number_of_dense_units=4096</em>, <em>dropout_rate=0.0</em>, <em>style=19</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_vgg_model.html#create_vgg_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_vgg_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the Vgg deep learning architecture.</p>
<p>Creates a keras model of the Vgg deep learning architecture for image
recognition based on the paper</p>
<p>K. Simonyan and A. Zisserman, Very Deep Convolutional Networks for
Large-Scale Image Recognition</p>
<p>available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://gist.github.com/baraldilorenzo/8d096f48a1be4a2d660d">https://gist.github.com/baraldilorenzo/8d096f48a1be4a2d660d</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of classification labels.</li>
<li><strong>layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – A tuple determining the number of ‘filters’ defined at for each layer.</li>
<li><strong>lowest_resolution</strong> (<em>python:integer</em>) – Number of filters at the beginning.</li>
<li><strong>convolution_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – 3-d vector definining the kernel size during the encoding path</li>
<li><strong>pool_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – 3-d vector defining the region for each pooling layer.</li>
<li><strong>strides</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – 3-d vector describing the stride length in each direction.</li>
<li><strong>number_of_dense_units</strong> (<em>python:integer</em>) – Number of units in the last layers.</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Between 0 and 1 to use between dense layers.</li>
<li><strong>style</strong> (<em>python:integer</em>) – ‘16’ or ‘19’ for VGG16 or VGG19, respectively.</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_vgg_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_resnet_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_resnet_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>layers=(1</em>, <em>2</em>, <em>3</em>, <em>4)</em>, <em>residual_block_schedule=(3</em>, <em>4</em>, <em>6</em>, <em>3)</em>, <em>lowest_resolution=64</em>, <em>cardinality=1</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_resnet_model.html#create_resnet_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_resnet_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the ResNet deep learning architecture.</p>
<p>Creates a keras model of the ResNet deep learning architecture for image
classification.  The paper is available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://gist.github.com/mjdietzx/0cb95922aac14d446a6530f87b3a04ce">https://gist.github.com/mjdietzx/0cb95922aac14d446a6530f87b3a04ce</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of classification labels.</li>
<li><strong>layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – A tuple determining the number of ‘filters’ defined at for each layer.</li>
<li><strong>residual_block_schedule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – A tuple defining the how many residual blocks repeats for each layer.</li>
<li><strong>lowest_resolution</strong> (<em>python:integer</em>) – Number of filters at the initial layer.</li>
<li><strong>cardinality</strong> (<em>python:integer</em>) – perform ResNet (cardinality = 1) or ResNeX (cardinality does not 1 but,
instead, powers of 2—try ‘32’).</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_resnet_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_resnet_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_resnet_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>layers=(1</em>, <em>2</em>, <em>3</em>, <em>4)</em>, <em>residual_block_schedule=(3</em>, <em>4</em>, <em>6</em>, <em>3)</em>, <em>lowest_resolution=64</em>, <em>cardinality=1</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_resnet_model.html#create_resnet_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_resnet_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the ResNet deep learning architecture.</p>
<p>Creates a keras model of the ResNet deep learning architecture for image
classification.  The paper is available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://gist.github.com/mjdietzx/0cb95922aac14d446a6530f87b3a04ce">https://gist.github.com/mjdietzx/0cb95922aac14d446a6530f87b3a04ce</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of classification labels.</li>
<li><strong>layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – A tuple determining the number of ‘filters’ defined at for each layer.</li>
<li><strong>residual_block_schedule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – A tuple defining the how many residual blocks repeats for each layer.</li>
<li><strong>lowest_resolution</strong> (<em>python:integer</em>) – Number of filters at the initial layer.</li>
<li><strong>cardinality</strong> (<em>python:integer</em>) – perform ResNet (cardinality = 1) or ResNeX (cardinality does not 1 but,
instead, powers of 2—try ‘32’).</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_resnet_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_wide_resnet_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_wide_resnet_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>depth=2</em>, <em>width=1</em>, <em>residual_block_schedule=(16</em>, <em>32</em>, <em>64)</em>, <em>pool_size=(8</em>, <em>8)</em>, <em>dropout_rate=0.0</em>, <em>weight_decay=0.0005</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_wide_resnet_model.html#create_wide_resnet_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_wide_resnet_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the Wide ResNet deep learning architecture.</p>
<p>Creates a keras model of the Wide ResNet deep learning architecture for image
classification/regression.  The paper is available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/titu1994/Wide-Residual-Networks">https://github.com/titu1994/Wide-Residual-Networks</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of classification labels.</li>
<li><strong>depth</strong> (<em>python:integer</em>) – Determines the depth of the network.  Related to the actual number of
layers by number_of_layers = depth * 6 + 4.    Default = 2 (such that
number_of_layers = 16).</li>
<li><strong>width</strong> (<em>python:integer</em>) – Determines the width of the network.  Default = 1.</li>
<li><strong>residual_block_schedule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Determines the number of filters per convolutional block. Default =
(16, 32, 64).</li>
<li><strong>pool_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Pool size for final average pooling layer.  Default = (8, 8).</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Float between 0 and 1 to use between dense layers.</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weighting parameter for regularization of the kernel weights of the
convolution layers.  Default = 0.0005.</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_wide_resnet_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_wide_resnet_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_wide_resnet_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>depth=2</em>, <em>width=1</em>, <em>residual_block_schedule=(16</em>, <em>32</em>, <em>64)</em>, <em>pool_size=(8</em>, <em>8</em>, <em>8)</em>, <em>dropout_rate=0.0</em>, <em>weight_decay=0.0005</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_wide_resnet_model.html#create_wide_resnet_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_wide_resnet_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the Wide ResNet deep learning architecture.</p>
<p>Creates a keras model of the Wide ResNet deep learning architecture for image
classification/regression.  The paper is available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/titu1994/Wide-Residual-Networks">https://github.com/titu1994/Wide-Residual-Networks</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of classification labels.</li>
<li><strong>depth</strong> (<em>python:integer</em>) – Determines the depth of the network.  Related to the actual number of
layers by number_of_layers = depth * 6 + 4.    Default = 2 (such that
number_of_layers = 16).</li>
<li><strong>width</strong> (<em>python:integer</em>) – Determines the width of the network.  Default = 1.</li>
<li><strong>residual_block_schedule</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Determines the number of filters per convolutional block. Default =
(16, 32, 64).</li>
<li><strong>pool_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Pool size for final average pooling layer.  Default = (8, 8, 8).</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Float between 0 and 1 to use between dense layers.</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weighting parameter for regularization of the kernel weights of the
convolution layers.  Default = 0.0005.</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_wide_resnet_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_densenet_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_densenet_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>number_of_filters=16</em>, <em>depth=7</em>, <em>number_of_dense_blocks=1</em>, <em>growth_rate=12</em>, <em>dropout_rate=0.2</em>, <em>weight_decay=0.0001</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_densenet_model.html#create_densenet_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_densenet_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the Wide ResNet deep learning architecture.</p>
<p>Creates a keras model of the DenseNet deep learning architecture for image
recognition based on the paper</p>
<p>G. Huang, Z. Liu, K. Weinberger, and L. van der Maaten. Densely Connected
Convolutional Networks Networks</p>
<p>available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1608.06993">https://arxiv.org/abs/1608.06993</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/DenseNet/densenet.py">https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/DenseNet/densenet.py</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of classification labels.</li>
<li><strong>number_of_filters</strong> (<em>python:integer</em>) – Number of filters.</li>
<li><strong>depth</strong> (<em>python:integer</em>) – Number of layers—must be equal to 3 * N + 4 where N is an integer (default = 7).</li>
<li><strong>number_of_dense_blocks</strong> (<em>python:integer</em>) – Number of dense blocks number of dense blocks to add to the end (default = 1).</li>
<li><strong>growth_rate</strong> (<em>python:integer</em>) – Number of filters to add for each dense block layer (default = 12).</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Per drop out layer rate (default = 0.2).</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weight decay (default = 1e-4).</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_densenet_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_densenet_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_densenet_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_classification_labels=1000</em>, <em>number_of_filters=16</em>, <em>depth=7</em>, <em>number_of_dense_blocks=1</em>, <em>growth_rate=12</em>, <em>dropout_rate=0.2</em>, <em>weight_decay=0.0001</em>, <em>mode='classification'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_densenet_model.html#create_densenet_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_densenet_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the Wide ResNet deep learning architecture.</p>
<p>Creates a keras model of the DenseNet deep learning architecture for image
recognition based on the paper</p>
<p>G. Huang, Z. Liu, K. Weinberger, and L. van der Maaten. Densely Connected
Convolutional Networks Networks</p>
<p>available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1608.06993">https://arxiv.org/abs/1608.06993</a></div></blockquote>
<p>This particular implementation was influenced by the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/DenseNet/densenet.py">https://github.com/tdeboissiere/DeepLearningImplementations/blob/master/DenseNet/densenet.py</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of classification labels.</li>
<li><strong>number_of_filters</strong> (<em>python:integer</em>) – Number of filters.</li>
<li><strong>depth</strong> (<em>python:integer</em>) – Number of layers—must be equal to 3 * N + 4 where N is an integer (default = 7).</li>
<li><strong>number_of_dense_blocks</strong> (<em>python:integer</em>) – Number of dense blocks number of dense blocks to add to the end (default = 1).</li>
<li><strong>growth_rate</strong> (<em>python:integer</em>) – Number of filters to add for each dense block layer (default = 12).</li>
<li><strong>dropout_rate</strong> (<em>scalar</em>) – Per drop out layer rate (default = 0.2).</li>
<li><strong>weight_decay</strong> (<em>scalar</em>) – Weight decay (default = 1e-4).</li>
<li><strong>mode</strong> (<em>string</em>) – ‘classification’ or ‘regression’.  Default = ‘classification’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_densenet_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_simple_classification_with_spatial_transformer_network_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_simple_classification_with_spatial_transformer_network_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>resampled_size=(30</em>, <em>30)</em>, <em>number_of_classification_labels=10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_simple_classification_with_spatial_transformer_network_model.html#create_simple_classification_with_spatial_transformer_network_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_simple_classification_with_spatial_transformer_network_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the spatial transformer network.</p>
<p>Creates a keras model of the spatial transformer network:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1506.02025">https://arxiv.org/abs/1506.02025</a></div></blockquote>
<p>based on the following python Keras model:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/oarriaga/STN.keras/blob/master/src/models/STN.py">https://github.com/oarriaga/STN.keras/blob/master/src/models/STN.py</a></div></blockquote>
<p>&#64;param inputImageSize Used for specifying the input tensor shape.  The
shape (or dimension) of that tensor is the image dimensions followed by
the number of channels (e.g., red, green, and blue).  The batch size
(i.e., number of training images) is not specified a priori.
&#64;param resampledSize resampled size of the transformed input images.
&#64;param numberOfClassificationLabels Number of classes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>resampled_size</strong> (<em>tuple of length 2</em>) – Resampled size of the transformed input images.</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of units in the final dense layer.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_simple_classification_with_spatial_transformer_network_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_simple_classification_with_spatial_transformer_network_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_simple_classification_with_spatial_transformer_network_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>resampled_size=(30</em>, <em>30</em>, <em>30)</em>, <em>number_of_classification_labels=10</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_simple_classification_with_spatial_transformer_network_model.html#create_simple_classification_with_spatial_transformer_network_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_simple_classification_with_spatial_transformer_network_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the spatial transformer network.</p>
<p>Creates a keras model of the spatial transformer network:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1506.02025">https://arxiv.org/abs/1506.02025</a></div></blockquote>
<p>based on the following python Keras model:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/oarriaga/STN.keras/blob/master/src/models/STN.py">https://github.com/oarriaga/STN.keras/blob/master/src/models/STN.py</a></div></blockquote>
<p>&#64;param inputImageSize Used for specifying the input tensor shape.  The
shape (or dimension) of that tensor is the image dimensions followed by
the number of channels (e.g., red, green, and blue).  The batch size
(i.e., number of training images) is not specified a priori.
&#64;param resampledSize resampled size of the transformed input images.
&#64;param numberOfClassificationLabels Number of classes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>resampled_size</strong> (<em>tuple of length 3</em>) – Resampled size of the transformed input images.</li>
<li><strong>number_of_classification_labels</strong> (<em>python:integer</em>) – Number of units in the final dense layer.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_simple_classification_with_spatial_transformer_network_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="image-super-resolution">
<h2>Image super-resolution<a class="headerlink" href="#image-super-resolution" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="antspynet.architectures.create_image_super_resolution_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_image_super_resolution_model_2d</code><span class="sig-paren">(</span><em>input_image_size, convolution_kernel_sizes=[(9, 9), (1, 1), (5, 5)], number_of_filters=(64, 32)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_image_super_resolution_model.html#create_image_super_resolution_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_image_super_resolution_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the image super resolution deep learning architecture.</p>
<p>Creates a keras model of the image super resolution deep learning framework.
based on the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/pdf/1501.00092">https://arxiv.org/pdf/1501.00092</a></div></blockquote>
<p>This particular implementation is based on the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/titu1994/Image-Super-Resolution">https://github.com/titu1994/Image-Super-Resolution</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>convolution_kernel_sizes</strong> (<em>list of 2-d tuples</em>) – specifies the kernel size at each convolution layer.  Default values are
the same as given in the original paper.  The length of kernel size list
must be 1 greater than the tuple length of the number of filters.</li>
<li><strong>number_of_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Contains the number of filters for each convolutional layer.  Default values
are the same as given in the original paper.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_image_super_resolution_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_image_super_resolution_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_image_super_resolution_model_3d</code><span class="sig-paren">(</span><em>input_image_size, convolution_kernel_sizes=[(9, 9, 9), (1, 1, 1), (5, 5, 5)], number_of_filters=(64, 32)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_image_super_resolution_model.html#create_image_super_resolution_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_image_super_resolution_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the image super resolution deep learning architecture.</p>
<p>Creates a keras model of the image super resolution deep learning framework.
based on the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/pdf/1501.00092">https://arxiv.org/pdf/1501.00092</a></div></blockquote>
<p>This particular implementation is based on the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/titu1994/Image-Super-Resolution">https://github.com/titu1994/Image-Super-Resolution</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>convolution_kernel_sizes</strong> (<em>list of 3-d tuples</em>) – specifies the kernel size at each convolution layer.  Default values are
the same as given in the original paper.  The length of kernel size list
must be 1 greater than the tuple length of the number of filters.</li>
<li><strong>number_of_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Contains the number of filters for each convolutional layer.  Default values
are the same as given in the original paper.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_image_super_resolution_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_expanded_super_resolution_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_expanded_super_resolution_model_2d</code><span class="sig-paren">(</span><em>input_image_size, convolution_kernel_sizes=[(9, 9), (1, 1), (3, 3), (5, 5), (5, 5)], number_of_filters=(64, 32, 32, 32)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_expanded_super_resolution_model.html#create_expanded_super_resolution_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_expanded_super_resolution_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the expanded  image super resolution deep learning architecture.</p>
<p>Creates a keras model of the image super resolution deep learning framework.
based on the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/pdf/1501.00092">https://arxiv.org/pdf/1501.00092</a></div></blockquote>
<p>This particular implementation is based on the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/titu1994/Image-Super-Resolution">https://github.com/titu1994/Image-Super-Resolution</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>convolution_kernel_sizes</strong> (<em>list of 2-d tuples</em>) – specifies the kernel size at each convolution layer.  Default values are
the same as given in the original paper.  The length of kernel size list
must be 1 greater than the tuple length of the number of filters.</li>
<li><strong>number_of_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Contains the number of filters for each convolutional layer.  Default values
are the same as given in the original paper.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_expanded_super_resolution_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_expanded_super_resolution_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_expanded_super_resolution_model_3d</code><span class="sig-paren">(</span><em>input_image_size, convolution_kernel_sizes=[(9, 9, 9), (1, 1, 1), (3, 3, 3), (5, 5, 5), (5, 5, 5)], number_of_filters=(64, 32, 32, 32)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_expanded_super_resolution_model.html#create_expanded_super_resolution_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_expanded_super_resolution_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the expanded  image super resolution deep learning architecture.</p>
<p>Creates a keras model of the image super resolution deep learning framework.
based on the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/pdf/1501.00092">https://arxiv.org/pdf/1501.00092</a></div></blockquote>
<p>This particular implementation is based on the following python
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/titu1994/Image-Super-Resolution">https://github.com/titu1994/Image-Super-Resolution</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>convolution_kernel_sizes</strong> (<em>list of 3-d tuples</em>) – specifies the kernel size at each convolution layer.  Default values are
the same as given in the original paper.  The length of kernel size list
must be 1 greater than the tuple length of the number of filters.</li>
<li><strong>number_of_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><em>tuple</em></a>) – Contains the number of filters for each convolutional layer.  Default values
are the same as given in the original paper.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_expanded_super_resolution_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_denoising_auto_encoder_super_resolution_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_denoising_auto_encoder_super_resolution_model_2d</code><span class="sig-paren">(</span><em>input_image_size, convolution_kernel_sizes=[(3, 3), (5, 5)], number_of_encoding_layers=2, number_of_filters=64</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_denoising_auto_encoder_super_resolution_model.html#create_denoising_auto_encoder_super_resolution_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_denoising_auto_encoder_super_resolution_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the denoising autoencoder image super resolution deep learning architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>convolution_kernel_sizes</strong> (<em>list of 2-d tuples</em>) – specifies the kernel size at each convolution layer.  Default values are
the same as given in the original paper.  The length of kernel size list
must be 1 greater than the tuple length of the number of filters.</li>
<li><strong>number_of_encoding_layers</strong> (<em>python:integer</em>) – The number of encoding layers.</li>
<li><strong>number_of_filters</strong> (<em>python:integer</em>) – The number of filters for each encoding layer.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_denoising_auto_encoder_super_resolution_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_denoising_auto_encoder_super_resolution_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_denoising_auto_encoder_super_resolution_model_3d</code><span class="sig-paren">(</span><em>input_image_size, convolution_kernel_sizes=[(3, 3, 3), (5, 5, 5)], number_of_encoding_layers=2, number_of_filters=64</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_denoising_auto_encoder_super_resolution_model.html#create_denoising_auto_encoder_super_resolution_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_denoising_auto_encoder_super_resolution_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the denoising autoencoder image super resolution deep learning architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>convolution_kernel_sizes</strong> (<em>list of 3-d tuples</em>) – specifies the kernel size at each convolution layer.  Default values are
the same as given in the original paper.  The length of kernel size list
must be 1 greater than the tuple length of the number of filters.</li>
<li><strong>number_of_encoding_layers</strong> (<em>python:integer</em>) – The number of encoding layers.</li>
<li><strong>number_of_filters</strong> (<em>python:integer</em>) – The number of filters for each encoding layer.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_denoising_auto_encoder_super_resolution_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_deep_denoise_super_resolution_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_deep_denoise_super_resolution_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>layers=2</em>, <em>lowest_resolution=64</em>, <em>convolution_kernel_size=(3</em>, <em>3)</em>, <em>pool_size=(2</em>, <em>2)</em>, <em>strides=(2</em>, <em>2)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_deep_denoise_super_resolution_model.html#create_deep_denoise_super_resolution_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_deep_denoise_super_resolution_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the denoising autoencoder image super resolution deep learning architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>layers</strong> (<em>python:integer</em>) – Number of architecture layers.</li>
<li><strong>lowest_resolution</strong> (<em>python:integer</em>) – Number of filters at the beginning and end of the architecture.</li>
<li><strong>convolution_kernel_size</strong> (<em>2-d tuple</em>) – specifies the kernel size during the encoding path.</li>
<li><strong>pool_size</strong> (<em>2-d tuple</em>) – Defines the region for each pooling layer.</li>
<li><strong>strides</strong> (<em>2-d tuple</em>) – Defines the stride length in each direction.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_deep_denoise_super_resolution_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_deep_denoise_super_resolution_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_deep_denoise_super_resolution_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>layers=2</em>, <em>lowest_resolution=64</em>, <em>convolution_kernel_size=(3</em>, <em>3</em>, <em>3)</em>, <em>pool_size=(2</em>, <em>2</em>, <em>2)</em>, <em>strides=(2</em>, <em>2</em>, <em>2)</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_deep_denoise_super_resolution_model.html#create_deep_denoise_super_resolution_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_deep_denoise_super_resolution_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the denoising autoencoder image super resolution deep learning architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>layers</strong> (<em>python:integer</em>) – Number of architecture layers.</li>
<li><strong>lowest_resolution</strong> (<em>python:integer</em>) – Number of filters at the beginning and end of the architecture.</li>
<li><strong>convolution_kernel_size</strong> (<em>3-d tuple</em>) – specifies the kernel size during the encoding path.</li>
<li><strong>pool_size</strong> (<em>3-d tuple</em>) – Defines the region for each pooling layer.</li>
<li><strong>strides</strong> (<em>3-d tuple</em>) – Defines the stride length in each direction.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_deep_denoise_super_resolution_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_resnet_super_resolution_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_resnet_super_resolution_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>convolution_kernel_size=(3</em>, <em>3)</em>, <em>number_of_filters=64</em>, <em>number_of_residual_blocks=5</em>, <em>number_of_resnet_blocks=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_resnet_super_resolution_model.html#create_resnet_super_resolution_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_resnet_super_resolution_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the ResNet image super resolution architecture.</p>
<p>Creates a keras model of the expanded image super resolution deep learning
framework based on the following python implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/titu1994/Image-Super-Resolution">https://github.com/titu1994/Image-Super-Resolution</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>convolution_kernel_size</strong> (<em>2-d tuple</em>) – Specifies the kernel size</li>
<li><strong>number_of_filters</strong> (<em>python:integer</em>) – The number of filters for each encoding layer.</li>
<li><strong>number_of_residual_blocks</strong> (<em>python:integer</em>) – Number of residual blocks.</li>
<li><strong>number_of_resnet_blocks</strong> (<em>python:integer</em>) – Number of resnet blocks.  Each block will double the upsampling amount.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_resnet_super_resolution_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_resnet_super_resolution_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_resnet_super_resolution_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>convolution_kernel_size=(3</em>, <em>3</em>, <em>3)</em>, <em>number_of_filters=64</em>, <em>number_of_residual_blocks=5</em>, <em>number_of_resnet_blocks=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_resnet_super_resolution_model.html#create_resnet_super_resolution_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_resnet_super_resolution_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the ResNet image super resolution architecture.</p>
<p>Creates a keras model of the expanded image super resolution deep learning
framework based on the following python implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/titu1994/Image-Super-Resolution">https://github.com/titu1994/Image-Super-Resolution</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>convolution_kernel_size</strong> (<em>3-d tuple</em>) – Specifies the kernel size</li>
<li><strong>number_of_filters</strong> (<em>python:integer</em>) – The number of filters for each encoding layer.</li>
<li><strong>number_of_residual_blocks</strong> (<em>python:integer</em>) – Number of residual blocks.</li>
<li><strong>number_of_resnet_blocks</strong> (<em>python:integer</em>) – Number of resnet blocks.  Each block will double the upsampling amount.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_resnet_super_resolution_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_deep_back_projection_network_model_2d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_deep_back_projection_network_model_2d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_outputs=1</em>, <em>number_of_base_filters=64</em>, <em>number_of_feature_filters=256</em>, <em>number_of_back_projection_stages=7</em>, <em>convolution_kernel_size=(12</em>, <em>12)</em>, <em>strides=(8</em>, <em>8)</em>, <em>last_convolution=(3</em>, <em>3)</em>, <em>number_of_loss_functions=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_deep_back_projection_network_model.html#create_deep_back_projection_network_model_2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_deep_back_projection_network_model_2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2-D implementation of the deep back-projection network.</p>
<p>Creates a keras model of the deep back-project network for image super
resolution.  More information is provided at the authors’ website:</p>
<blockquote>
<div><a class="reference external" href="https://www.toyota-ti.ac.jp/Lab/Denshi/iim/members/muhammad.haris/projects/DBPN.html">https://www.toyota-ti.ac.jp/Lab/Denshi/iim/members/muhammad.haris/projects/DBPN.html</a></div></blockquote>
<p>with the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1803.02735">https://arxiv.org/abs/1803.02735</a></div></blockquote>
<p>This particular implementation was influenced by the following keras (python)
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/rajatkb/DBPN-Keras">https://github.com/rajatkb/DBPN-Keras</a></div></blockquote>
<p>with help from the original author’s Caffe and Pytorch implementations:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/alterzero/DBPN-caffe">https://github.com/alterzero/DBPN-caffe</a>
<a class="reference external" href="https://github.com/alterzero/DBPN-Pytorch">https://github.com/alterzero/DBPN-Pytorch</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 3</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_outputs</strong> (<em>python:integer</em>) – Number of outputs (e.g., 3 for RGB images).</li>
<li><strong>number_of_feature_filters</strong> (<em>python:integer</em>) – Number of feature filters.</li>
<li><strong>number_of_base_filters</strong> (<em>python:integer</em>) – Number of base filters.</li>
<li><strong>number_of_back_projection_stages</strong> (<em>python:integer</em>) – Number of up-down-projection stages.
This number includes the final up block.</li>
<li><strong>convolution_kernel_size</strong> (<em>tuple of length 2</em>) – Kernel size for certain convolutional layers.  The strides are dependent on
the scale factor discussed in original paper.  Factors used in the original
implementation are as follows:
2x –&gt; convolution_kernel_size=(6, 6),
4x –&gt; convolution_kernel_size=(8, 8),
8x –&gt; convolution_kernel_size=(12, 12).  We default to 8x parameters.</li>
<li><strong>strides</strong> (<em>tuple of length 2</em>) – Strides for certain convolutional layers.  This and the
convolution_kernel_size are dependent on the scale factor discussed in
original paper.  Factors used in the original implementation are as follows:
2x –&gt; strides = (2, 2),
4x –&gt; strides = (4, 4),
8x –&gt; strides = (8, 8). We default to 8x parameters.</li>
<li><strong>last_convolution</strong> (<em>tuple of length 2</em>) – The kernel size for the last convolutional layer.</li>
<li><strong>number_of_loss_functions</strong> (<em>python:integer</em>) – The number of data targets, e.g. 2 for 2 targets</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 2-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_deep_back_projection_network_model_2d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="antspynet.architectures.create_deep_back_projection_network_model_3d">
<code class="descclassname">antspynet.architectures.</code><code class="descname">create_deep_back_projection_network_model_3d</code><span class="sig-paren">(</span><em>input_image_size</em>, <em>number_of_outputs=1</em>, <em>number_of_base_filters=64</em>, <em>number_of_feature_filters=256</em>, <em>number_of_back_projection_stages=7</em>, <em>convolution_kernel_size=(12</em>, <em>12</em>, <em>12)</em>, <em>strides=(8</em>, <em>8</em>, <em>8)</em>, <em>last_convolution=(3</em>, <em>3</em>, <em>3)</em>, <em>number_of_loss_functions=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/antspynet/architectures/create_deep_back_projection_network_model.html#create_deep_back_projection_network_model_3d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#antspynet.architectures.create_deep_back_projection_network_model_3d" title="Permalink to this definition">¶</a></dt>
<dd><p>3-D implementation of the deep back-projection network.</p>
<p>Creates a keras model of the deep back-project network for image super
resolution.  More information is provided at the authors’ website:</p>
<blockquote>
<div><a class="reference external" href="https://www.toyota-ti.ac.jp/Lab/Denshi/iim/members/muhammad.haris/projects/DBPN.html">https://www.toyota-ti.ac.jp/Lab/Denshi/iim/members/muhammad.haris/projects/DBPN.html</a></div></blockquote>
<p>with the paper available here:</p>
<blockquote>
<div><a class="reference external" href="https://arxiv.org/abs/1803.02735">https://arxiv.org/abs/1803.02735</a></div></blockquote>
<p>This particular implementation was influenced by the following keras (python)
implementation:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/rajatkb/DBPN-Keras">https://github.com/rajatkb/DBPN-Keras</a></div></blockquote>
<p>with help from the original author’s Caffe and Pytorch implementations:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/alterzero/DBPN-caffe">https://github.com/alterzero/DBPN-caffe</a>
<a class="reference external" href="https://github.com/alterzero/DBPN-Pytorch">https://github.com/alterzero/DBPN-Pytorch</a></div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_image_size</strong> (<em>tuple of length 4</em>) – Used for specifying the input tensor shape.  The shape (or dimension) of
that tensor is the image dimensions followed by the number of channels
(e.g., red, green, and blue).</li>
<li><strong>number_of_outputs</strong> (<em>python:integer</em>) – Number of outputs (e.g., 3 for RGB images).</li>
<li><strong>number_of_feature_filters</strong> (<em>python:integer</em>) – Number of feature filters.</li>
<li><strong>number_of_base_filters</strong> (<em>python:integer</em>) – Number of base filters.</li>
<li><strong>number_of_back_projection_stages</strong> (<em>python:integer</em>) – Number of up-down-projection stages.
This number includes the final up block.</li>
<li><strong>convolution_kernel_size</strong> (<em>tuple of length 3</em>) – Kernel size for certain convolutional layers.  The strides are dependent on
the scale factor discussed in original paper.  Factors used in the original
implementation are as follows:
2x –&gt; convolution_kernel_size=(6, 6, 6),
4x –&gt; convolution_kernel_size=(8, 8, 8),
8x –&gt; convolution_kernel_size=(12, 12, 12).  We default to 8x parameters.</li>
<li><strong>strides</strong> (<em>tuple of length 3</em>) – Strides for certain convolutional layers.  This and the
convolution_kernel_size are dependent on the scale factor discussed in
original paper.  Factors used in the original implementation are as follows:
2x –&gt; strides = (2, 2, 2),
4x –&gt; strides = (4, 4, 4),
8x –&gt; strides = (8, 8, 8). We default to 8x parameters.</li>
<li><strong>last_convolution</strong> (<em>tuple of length 3</em>) – The kernel size for the last convolutional layer.</li>
<li><strong>number_of_loss_functions</strong> (<em>python:integer</em>) – The number of data targets, e.g. 2 for 2 targets</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A 3-D Keras model defining the network.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Keras model</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">create_deep_back_projection_network_model_3d</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">ANTsPyNet</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Architectures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#image-voxelwise-segmentation-regression">Image voxelwise segmentation/regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#image-classification-regression">Image classification/regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#image-super-resolution">Image super-resolution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utilities.html">Utilities</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">ANTsPyNet documentation</a></li>
      <li>Next: <a href="utilities.html" title="next chapter">Utilities</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Nick Tustison, Brian Avants, and Nick Cullen.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/architectures.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>